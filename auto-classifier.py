# -*- coding: utf-8 -*-
"""Clustering-guided LigthGBM Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Coecv8C7wvFAuJrgcaKqTABUexQlyff
"""

# install pycaret
!pip install pycaret

# create notebook kernel connected with the conda environment
#!python -m ipykernel install --user --name yourenvname --display-name "display-name"

!pip install Jinja2==3.1.2

!pip install -U cluster-over-sampling

!pip install shap

!pip install interpret-community

!pip install numpy==1.20

import pandas as pd

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_excel(io.BytesIO(uploaded['classification_model.xlsx']))

(df.columns.tolist())

df.isnull().sum()

df2 = df.fillna(0)

df2.isnull().sum()

df2 = df2.drop(['RING NU','Altitude','ExcavationD','pitching','rolling','Middle break left and right fold angle (%)','Middle break upper and lower folds (%)',' Geosanth exploration equipment exploration pressure (kN)',
              'Geoyama Exploration Equipment Exploration Stroke (mm)','Clay shock injection pressure (MPa)','Clay shock flow rateA (L/min)','Clay shock flow rateB (L/min)',
              'Back injection pressure (MPa)',' Rotation angle (degree)','Bubble injection pressure (MPa)','Back in flow rate of A liquid (L/min)','Back in flow rate of B liquid (L/min)','Excavated Tunnel Length (m)',],axis=1)

df2.head()

df2.describe()

"""#CLASS DISTRIBUTION IN RAW DATA

"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
#df = pd.read_csv('data.csv')

# Melt data into long format
df_long = pd.melt(df2, id_vars='Layers', var_name='Variable', value_name='Value')

# Create boxplot for each variable
plotlist = []
for var in df_long['Variable'].unique():
    plot = sns.boxplot(data=df_long[df_long['Variable']==var], x='Layers', y='Value', color='blue')
    plot.set_title(var)
    plotlist.append(plot)

# Arrange plots in grid
n = len(plotlist)
nrow = int(n**0.5)
ncol = int(n/nrow) + int(n%nrow>0)
fig, axes = plt.subplots(nrows=nrow, ncols=ncol, figsize=(ncol*16, nrow*16/1.5))
for i, ax in enumerate(axes.flat):
    if i < n:
        ax.set_aspect(1/1.5)
        ax = plotlist[i].axes
    else:
        ax.axis('off')

# Adjust space between plots and add overall title
fig.subplots_adjust(hspace=0.4, wspace=0.4)
fig.suptitle('Boxplots of Variables by Layers', fontsize=30, y=0.95)
plt.show()

df2.Layers.value_counts()

import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 600

"""#Before setting up PyCaret, a random sample of 10% size of the dataset will be get to make predictions with unseen data."""

RANDOM_SEED = 142

def data_sampling(dataset, frac: float, random_seed: int):
    data_sampled_a = dataset.sample(frac=frac, random_state=random_seed)
    data_sampled_b =  dataset.drop(data_sampled_a.index).reset_index(drop=True)
    data_sampled_a.reset_index(drop=True, inplace=True)
    return data_sampled_a, data_sampled_b

df2, data_unseen = data_sampling(df2, 0.9, RANDOM_SEED)
print(f"There are {data_unseen.shape[0]} samples for Unseen Data.")

"""#In order to demonstrate the use of the predict_model function on unseen data, a sample of 73 records (~10%) has been withheld from the original dataset to be used for predictions at the end. This should not be confused with a train-test-split."""

print('Data for Modeling: ' + str(df2.shape))
print('Unseen Data For Predictions: ' + str(data_unseen.shape))

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')
plt.rcParams['figure.dpi'] = 600

# --- Create List of Color Palletes ---
yellow_grad = ['#413D05', '#837A0B', '#C5B710', '#E8DA2E', '#F9EE6A']
olive_grad = ['#2A3F07', '#557F0F', '#7FBE17', '#A4E637', '#C1EE76']
color_mix = ['#356211', '#6C8F32', '#F2E880', '#C15C37', '#AA471F']
black_grad = ['#100C07', '#3E3B39', '#6D6A6A', '#9B9A9C', '#CAC9CD']

# --- Plot Color Palletes --
sns.palplot(yellow_grad)
sns.palplot(olive_grad)
sns.palplot(color_mix)
sns.palplot(black_grad)

# --- Colors and Labels (Null Values Dropped) ---
colors = color_mix[1:4]
labels = df2['Layers'].dropna().unique()
order = df2['Layers'].value_counts().index

# --- Size for Both Figures ---
plt.figure(figsize=(18, 10))

#plt.rcParams['figure.dpi'] = 600

# --- Pie Chart ---
plt.subplot(1, 2, 1)
plt.title('Tunnel Lithology Distribution', fontweight = 'bold', fontsize = 14, fontfamily = 'serif',
          color = black_grad[0])
plt.pie(df2['Layers'].value_counts(), labels = order, colors = colors,
        wedgeprops = dict(alpha = 0.8, edgecolor = black_grad[1]), autopct = '%.2f%%')


# --- Bar Chart ---
countplt = plt.subplot(1, 2, 2)
plt.title('Tunnel Lithology Distribution', fontweight = 'bold', fontsize = 14, fontfamily = 'sans-serif', color = black_grad[0])
ax = sns.countplot(x = 'Layers', data = df2, palette = colors, order = order, edgecolor = black_grad[2], alpha = 0.85)
for rect in ax.patches:
    ax.text (rect.get_x()+rect.get_width()/2, rect.get_height()+0.75,rect.get_height(), horizontalalignment = 'center',
             fontsize = 11)

plt.xlabel('Lithologies', fontweight = 'bold', fontsize = 11, fontfamily = 'serif', color = black_grad[1])
plt.ylabel('Total', fontweight = 'bold', fontsize = 11, fontfamily = 'serif', color = black_grad[1])
plt.grid(axis = 'y', alpha = 0.4)
countplt
plt.savefig("High resoltion.jpg",dpi=600)

# --- Count Categorical Labels w/out Dropping Null Walues ---
print('\033[92m'+'*' * 40)
print('\033[92m\033[1m'+'.: Tunnel Lithology Class Distribution after Unseen Data (10%) :.'+'\033[0m')
print('\033[92m'+'*' * 40+'\033[0m')
df2.Layers.value_counts(dropna=False)

df2.hist(bins = 30, figsize = (15,15), grid = False)
plt.show()

import seaborn as sns

df2.info()

"""#BALANCED DATA

#CLUSTERING-GUIDED LIGHTGBM CLASSIFIER

#The resampling is applied only during training and per fold, to remove the risk of data leakage. Therefore, any changes introduced by sampling methods will not be visible in the dataframes obtained through get.config()
"""

from numpy import NDArray

!pip install numpy==1.20

from sklearn.cluster import KMeans
from imblearn.over_sampling import SMOTE
from clover.over_sampling import ClusterOverSampler
from clover.distribution import DensityDistributor
from pycaret.classification import *

clovrs = ClusterOverSampler(oversampler=SMOTE(random_state=1), clusterer=KMeans(random_state=2), distributor=DensityDistributor(), random_state=3)

Session_2 = setup(df2, target = 'Layers', session_id=177, log_experiment=False,
                  experiment_name='lithologies2', normalize=True, normalize_method='minmax',
                  transformation=True, transformation_method = 'quantile', fix_imbalance=True,
                  fix_imbalance_method= clovrs, remove_multicollinearity=True)

get_config("X_train")

best_model1 = compare_models(sort="F1")

"""#LigthGBM Classifier"""

lightgbm_balanced = create_model('lightgbm', fold=5)

tuned_lightgbm_balanced = tune_model(lightgbm_balanced,fold=5, optimize="F1")

plot_model(tuned_lightgbm_balanced,plot='class_report',  plot_kwargs = {'title' : 'LightGBM Classifier Classification Report'})

plot_model(tuned_lightgbm_balanced,plot='confusion_matrix',  plot_kwargs = {'title' : 'LightGBM Classifier Confusion Matrix'})

interpret_model(tuned_lightgbm_balanced)

#Feature Importance Plot
plot_model(tuned_lightgbm_balanced, plot = 'feature')

unseen_data=predict_model(tuned_lightgbm_balanced, data=data_unseen)
unseen_data.head(10)

# finalize the model**
final_best = finalize_model(tuned_lightgbm_balanced)

# save model to disk
save_model(final_best, model_name='classifier-pipeline')

model=load_model("classifier-pipeline")